{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "neutral-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "corresponding-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,losses,metrics,callbacks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-playback",
   "metadata": {},
   "source": [
    "## Expiriment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "chronic-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "underlying-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev = pd.read_csv(\"../data/debug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "celtic-overhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>feature_79</th>\n",
       "      <th>feature_80</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.872746</td>\n",
       "      <td>-2.191242</td>\n",
       "      <td>-0.474163</td>\n",
       "      <td>-0.323046</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>-0.002484</td>\n",
       "      <td>0.264749</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>-0.989982</td>\n",
       "      <td>-1.055090</td>\n",
       "      <td>0.342371</td>\n",
       "      <td>-0.074321</td>\n",
       "      <td>-2.667671</td>\n",
       "      <td>-2.001475</td>\n",
       "      <td>-1.703595</td>\n",
       "      <td>-2.196892</td>\n",
       "      <td>0.15616</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>1.483295</td>\n",
       "      <td>1.307466</td>\n",
       "      <td>0.460831</td>\n",
       "      <td>0.173712</td>\n",
       "      <td>1.175200</td>\n",
       "      <td>0.967805</td>\n",
       "      <td>1.608410</td>\n",
       "      <td>1.319365</td>\n",
       "      <td>0.386854</td>\n",
       "      <td>0.235952</td>\n",
       "      <td>-0.515073</td>\n",
       "      <td>-0.448988</td>\n",
       "      <td>0.37585</td>\n",
       "      <td>0.174779</td>\n",
       "      <td>-2.429812</td>\n",
       "      <td>-2.206423</td>\n",
       "      <td>-3.593120</td>\n",
       "      <td>-2.868358</td>\n",
       "      <td>0.112697</td>\n",
       "      <td>0.053157</td>\n",
       "      <td>-0.539956</td>\n",
       "      <td>-0.692187</td>\n",
       "      <td>3.491282</td>\n",
       "      <td>-1.684889</td>\n",
       "      <td>1.337123</td>\n",
       "      <td>-0.328607</td>\n",
       "      <td>1.689207</td>\n",
       "      <td>-1.052243</td>\n",
       "      <td>-1.870885</td>\n",
       "      <td>-1.789342</td>\n",
       "      <td>-1.574173</td>\n",
       "      <td>-1.120820</td>\n",
       "      <td>-0.571920</td>\n",
       "      <td>-1.093033</td>\n",
       "      <td>0.703515</td>\n",
       "      <td>5.936281</td>\n",
       "      <td>0.954714</td>\n",
       "      <td>3.315812</td>\n",
       "      <td>1.291338</td>\n",
       "      <td>2.468825</td>\n",
       "      <td>2.490069</td>\n",
       "      <td>-1.148239</td>\n",
       "      <td>-0.961935</td>\n",
       "      <td>-2.263944</td>\n",
       "      <td>-2.158765</td>\n",
       "      <td>-5.012022</td>\n",
       "      <td>-2.006825</td>\n",
       "      <td>-1.284090</td>\n",
       "      <td>-2.141697</td>\n",
       "      <td>-2.054935</td>\n",
       "      <td>-1.851203</td>\n",
       "      <td>-1.431184</td>\n",
       "      <td>-1.634481</td>\n",
       "      <td>0.219379</td>\n",
       "      <td>-0.373934</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.559241</td>\n",
       "      <td>0.891368</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>-1.521125</td>\n",
       "      <td>-0.028488</td>\n",
       "      <td>3.045337</td>\n",
       "      <td>3.260512</td>\n",
       "      <td>0.683558</td>\n",
       "      <td>0.753033</td>\n",
       "      <td>-0.109194</td>\n",
       "      <td>0.409876</td>\n",
       "      <td>0.488806</td>\n",
       "      <td>1.447504</td>\n",
       "      <td>-2.790902</td>\n",
       "      <td>1.26094</td>\n",
       "      <td>1.158770</td>\n",
       "      <td>0.982421</td>\n",
       "      <td>3.754522</td>\n",
       "      <td>7.137163</td>\n",
       "      <td>-1.863069</td>\n",
       "      <td>0.701201</td>\n",
       "      <td>0.434466</td>\n",
       "      <td>0.423965</td>\n",
       "      <td>-0.292035</td>\n",
       "      <td>0.317003</td>\n",
       "      <td>-2.605820</td>\n",
       "      <td>1.360833</td>\n",
       "      <td>2.896986</td>\n",
       "      <td>0.915523</td>\n",
       "      <td>1.485813</td>\n",
       "      <td>4.147254</td>\n",
       "      <td>-2.238831</td>\n",
       "      <td>0.88024</td>\n",
       "      <td>-0.892724</td>\n",
       "      <td>0.389005</td>\n",
       "      <td>-0.156332</td>\n",
       "      <td>0.622816</td>\n",
       "      <td>-3.921523</td>\n",
       "      <td>1.286974</td>\n",
       "      <td>2.561593</td>\n",
       "      <td>1.189605</td>\n",
       "      <td>3.457757</td>\n",
       "      <td>6.649580</td>\n",
       "      <td>-1.472686</td>\n",
       "      <td>1.972286</td>\n",
       "      <td>2.08165</td>\n",
       "      <td>1.168391</td>\n",
       "      <td>8.313583</td>\n",
       "      <td>1.782433</td>\n",
       "      <td>14.018213</td>\n",
       "      <td>2.653056</td>\n",
       "      <td>12.600292</td>\n",
       "      <td>2.301488</td>\n",
       "      <td>11.445807</td>\n",
       "      <td>0</td>\n",
       "      <td>3.143516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16.673515</td>\n",
       "      <td>-0.002828</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>-0.007319</td>\n",
       "      <td>-0.011114</td>\n",
       "      <td>-0.009792</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.349537</td>\n",
       "      <td>-1.704709</td>\n",
       "      <td>0.068058</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.193794</td>\n",
       "      <td>0.138212</td>\n",
       "      <td>0.264749</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>-0.151877</td>\n",
       "      <td>-0.384952</td>\n",
       "      <td>0.342371</td>\n",
       "      <td>-0.074321</td>\n",
       "      <td>1.225838</td>\n",
       "      <td>0.789076</td>\n",
       "      <td>1.110580</td>\n",
       "      <td>1.102281</td>\n",
       "      <td>0.15616</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>-0.590600</td>\n",
       "      <td>-0.625682</td>\n",
       "      <td>0.460831</td>\n",
       "      <td>0.173712</td>\n",
       "      <td>-0.543425</td>\n",
       "      <td>-0.547486</td>\n",
       "      <td>-0.706600</td>\n",
       "      <td>-0.667806</td>\n",
       "      <td>0.386854</td>\n",
       "      <td>0.235952</td>\n",
       "      <td>0.910558</td>\n",
       "      <td>0.914465</td>\n",
       "      <td>0.37585</td>\n",
       "      <td>0.174779</td>\n",
       "      <td>2.137454</td>\n",
       "      <td>2.080459</td>\n",
       "      <td>2.819291</td>\n",
       "      <td>2.483965</td>\n",
       "      <td>-0.086755</td>\n",
       "      <td>-0.082687</td>\n",
       "      <td>0.368431</td>\n",
       "      <td>0.469196</td>\n",
       "      <td>5.711996</td>\n",
       "      <td>-2.215132</td>\n",
       "      <td>0.796703</td>\n",
       "      <td>-1.140081</td>\n",
       "      <td>0.716617</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.198920</td>\n",
       "      <td>-0.326697</td>\n",
       "      <td>-0.381770</td>\n",
       "      <td>1.435607</td>\n",
       "      <td>3.401393</td>\n",
       "      <td>2.486748</td>\n",
       "      <td>-2.014598</td>\n",
       "      <td>-0.390588</td>\n",
       "      <td>0.954714</td>\n",
       "      <td>-0.027262</td>\n",
       "      <td>-1.886927</td>\n",
       "      <td>-1.706450</td>\n",
       "      <td>-0.888236</td>\n",
       "      <td>-1.138294</td>\n",
       "      <td>-0.954461</td>\n",
       "      <td>-1.350633</td>\n",
       "      <td>-1.459546</td>\n",
       "      <td>-4.564815</td>\n",
       "      <td>-2.651966</td>\n",
       "      <td>-1.620014</td>\n",
       "      <td>-2.240625</td>\n",
       "      <td>-2.147273</td>\n",
       "      <td>-0.255224</td>\n",
       "      <td>3.202946</td>\n",
       "      <td>-0.535872</td>\n",
       "      <td>0.219379</td>\n",
       "      <td>-0.050948</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.141089</td>\n",
       "      <td>0.058363</td>\n",
       "      <td>0.131190</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>-0.121239</td>\n",
       "      <td>-0.028488</td>\n",
       "      <td>0.677553</td>\n",
       "      <td>0.045842</td>\n",
       "      <td>-0.124616</td>\n",
       "      <td>0.753033</td>\n",
       "      <td>-0.007004</td>\n",
       "      <td>0.409876</td>\n",
       "      <td>-0.410491</td>\n",
       "      <td>-0.024323</td>\n",
       "      <td>-3.012654</td>\n",
       "      <td>1.26094</td>\n",
       "      <td>1.157671</td>\n",
       "      <td>0.982421</td>\n",
       "      <td>1.297679</td>\n",
       "      <td>1.281956</td>\n",
       "      <td>-2.427595</td>\n",
       "      <td>0.701201</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.423965</td>\n",
       "      <td>-0.413607</td>\n",
       "      <td>-0.073672</td>\n",
       "      <td>-2.434546</td>\n",
       "      <td>1.360833</td>\n",
       "      <td>0.949879</td>\n",
       "      <td>0.915523</td>\n",
       "      <td>0.724655</td>\n",
       "      <td>1.622137</td>\n",
       "      <td>-2.209020</td>\n",
       "      <td>0.88024</td>\n",
       "      <td>-1.332492</td>\n",
       "      <td>0.389005</td>\n",
       "      <td>-0.586619</td>\n",
       "      <td>-1.040491</td>\n",
       "      <td>-3.946097</td>\n",
       "      <td>1.286974</td>\n",
       "      <td>0.983440</td>\n",
       "      <td>1.189605</td>\n",
       "      <td>1.357907</td>\n",
       "      <td>1.612348</td>\n",
       "      <td>-1.664544</td>\n",
       "      <td>1.972286</td>\n",
       "      <td>2.08165</td>\n",
       "      <td>-1.178850</td>\n",
       "      <td>1.777472</td>\n",
       "      <td>-0.915458</td>\n",
       "      <td>2.831612</td>\n",
       "      <td>-1.417010</td>\n",
       "      <td>2.297459</td>\n",
       "      <td>-1.304614</td>\n",
       "      <td>1.898684</td>\n",
       "      <td>1</td>\n",
       "      <td>4.293568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.027607</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>0.034380</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.812780</td>\n",
       "      <td>-0.256156</td>\n",
       "      <td>0.806463</td>\n",
       "      <td>0.400221</td>\n",
       "      <td>-0.614188</td>\n",
       "      <td>-0.354800</td>\n",
       "      <td>0.264749</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>5.448261</td>\n",
       "      <td>2.668029</td>\n",
       "      <td>0.342371</td>\n",
       "      <td>-0.074321</td>\n",
       "      <td>3.836342</td>\n",
       "      <td>2.183258</td>\n",
       "      <td>3.902698</td>\n",
       "      <td>3.045431</td>\n",
       "      <td>0.15616</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>-1.141082</td>\n",
       "      <td>-0.979962</td>\n",
       "      <td>0.460831</td>\n",
       "      <td>0.173712</td>\n",
       "      <td>-1.157585</td>\n",
       "      <td>-0.966803</td>\n",
       "      <td>-1.430973</td>\n",
       "      <td>-1.103432</td>\n",
       "      <td>0.386854</td>\n",
       "      <td>0.235952</td>\n",
       "      <td>5.131559</td>\n",
       "      <td>4.314714</td>\n",
       "      <td>0.37585</td>\n",
       "      <td>0.174779</td>\n",
       "      <td>4.226341</td>\n",
       "      <td>3.173640</td>\n",
       "      <td>5.991513</td>\n",
       "      <td>4.142298</td>\n",
       "      <td>-0.167927</td>\n",
       "      <td>-0.124778</td>\n",
       "      <td>0.749326</td>\n",
       "      <td>0.715824</td>\n",
       "      <td>-0.039007</td>\n",
       "      <td>0.186321</td>\n",
       "      <td>2.323887</td>\n",
       "      <td>0.162261</td>\n",
       "      <td>0.237987</td>\n",
       "      <td>-0.350221</td>\n",
       "      <td>-0.138033</td>\n",
       "      <td>-0.516281</td>\n",
       "      <td>-0.703543</td>\n",
       "      <td>-0.556954</td>\n",
       "      <td>-0.816910</td>\n",
       "      <td>-0.455841</td>\n",
       "      <td>2.383226</td>\n",
       "      <td>3.474416</td>\n",
       "      <td>0.954714</td>\n",
       "      <td>0.883247</td>\n",
       "      <td>-0.084247</td>\n",
       "      <td>0.622561</td>\n",
       "      <td>0.953619</td>\n",
       "      <td>-1.128774</td>\n",
       "      <td>-0.946507</td>\n",
       "      <td>-1.470762</td>\n",
       "      <td>-1.594587</td>\n",
       "      <td>-4.346199</td>\n",
       "      <td>-2.276678</td>\n",
       "      <td>-1.417652</td>\n",
       "      <td>-2.166362</td>\n",
       "      <td>-2.077110</td>\n",
       "      <td>0.692339</td>\n",
       "      <td>0.467898</td>\n",
       "      <td>-0.297919</td>\n",
       "      <td>0.219379</td>\n",
       "      <td>-0.463646</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.129187</td>\n",
       "      <td>-0.426321</td>\n",
       "      <td>0.261728</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>-0.456567</td>\n",
       "      <td>-0.028488</td>\n",
       "      <td>0.192444</td>\n",
       "      <td>-0.423503</td>\n",
       "      <td>0.309331</td>\n",
       "      <td>0.753033</td>\n",
       "      <td>1.255440</td>\n",
       "      <td>0.409876</td>\n",
       "      <td>0.525988</td>\n",
       "      <td>0.934815</td>\n",
       "      <td>-2.881893</td>\n",
       "      <td>1.26094</td>\n",
       "      <td>2.420089</td>\n",
       "      <td>0.982421</td>\n",
       "      <td>0.800962</td>\n",
       "      <td>1.143663</td>\n",
       "      <td>-3.214578</td>\n",
       "      <td>0.701201</td>\n",
       "      <td>1.585939</td>\n",
       "      <td>0.423965</td>\n",
       "      <td>0.193996</td>\n",
       "      <td>0.953114</td>\n",
       "      <td>-2.674838</td>\n",
       "      <td>1.360833</td>\n",
       "      <td>2.200085</td>\n",
       "      <td>0.915523</td>\n",
       "      <td>0.537175</td>\n",
       "      <td>2.156228</td>\n",
       "      <td>-3.568648</td>\n",
       "      <td>0.88024</td>\n",
       "      <td>1.193823</td>\n",
       "      <td>0.389005</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.796214</td>\n",
       "      <td>-4.090058</td>\n",
       "      <td>1.286974</td>\n",
       "      <td>2.548596</td>\n",
       "      <td>1.189605</td>\n",
       "      <td>0.882588</td>\n",
       "      <td>1.817895</td>\n",
       "      <td>-2.432424</td>\n",
       "      <td>1.972286</td>\n",
       "      <td>2.08165</td>\n",
       "      <td>6.115747</td>\n",
       "      <td>9.667908</td>\n",
       "      <td>5.542871</td>\n",
       "      <td>11.671595</td>\n",
       "      <td>7.281757</td>\n",
       "      <td>10.060014</td>\n",
       "      <td>6.638248</td>\n",
       "      <td>9.427299</td>\n",
       "      <td>2</td>\n",
       "      <td>2.471201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004730</td>\n",
       "      <td>-0.003273</td>\n",
       "      <td>-0.000461</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.174378</td>\n",
       "      <td>0.344640</td>\n",
       "      <td>0.066872</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>-1.006373</td>\n",
       "      <td>-0.676458</td>\n",
       "      <td>0.264749</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>4.508206</td>\n",
       "      <td>2.484260</td>\n",
       "      <td>0.342371</td>\n",
       "      <td>-0.074321</td>\n",
       "      <td>2.902176</td>\n",
       "      <td>1.799163</td>\n",
       "      <td>3.192700</td>\n",
       "      <td>2.848359</td>\n",
       "      <td>0.15616</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>-1.401637</td>\n",
       "      <td>-1.428248</td>\n",
       "      <td>0.460831</td>\n",
       "      <td>0.173712</td>\n",
       "      <td>-1.421175</td>\n",
       "      <td>-1.487976</td>\n",
       "      <td>-1.756415</td>\n",
       "      <td>-1.647543</td>\n",
       "      <td>0.386854</td>\n",
       "      <td>0.235952</td>\n",
       "      <td>4.766182</td>\n",
       "      <td>4.528353</td>\n",
       "      <td>0.37585</td>\n",
       "      <td>0.174779</td>\n",
       "      <td>3.330068</td>\n",
       "      <td>2.778468</td>\n",
       "      <td>5.603940</td>\n",
       "      <td>4.343171</td>\n",
       "      <td>-0.203161</td>\n",
       "      <td>-0.177835</td>\n",
       "      <td>0.642206</td>\n",
       "      <td>0.694692</td>\n",
       "      <td>-0.607811</td>\n",
       "      <td>2.718151</td>\n",
       "      <td>1.656999</td>\n",
       "      <td>0.192241</td>\n",
       "      <td>-0.622152</td>\n",
       "      <td>-0.024920</td>\n",
       "      <td>0.868425</td>\n",
       "      <td>0.414826</td>\n",
       "      <td>0.064472</td>\n",
       "      <td>-0.752845</td>\n",
       "      <td>-0.560471</td>\n",
       "      <td>-0.455841</td>\n",
       "      <td>2.979093</td>\n",
       "      <td>0.770532</td>\n",
       "      <td>0.954714</td>\n",
       "      <td>0.574002</td>\n",
       "      <td>0.081969</td>\n",
       "      <td>0.840800</td>\n",
       "      <td>0.794274</td>\n",
       "      <td>-1.127171</td>\n",
       "      <td>-0.945261</td>\n",
       "      <td>-1.416144</td>\n",
       "      <td>-1.531585</td>\n",
       "      <td>-4.322755</td>\n",
       "      <td>-3.377519</td>\n",
       "      <td>-2.010178</td>\n",
       "      <td>-2.190485</td>\n",
       "      <td>-2.099841</td>\n",
       "      <td>2.081633</td>\n",
       "      <td>-0.283574</td>\n",
       "      <td>0.938397</td>\n",
       "      <td>0.219379</td>\n",
       "      <td>2.837515</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>1.757084</td>\n",
       "      <td>2.730964</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>1.206258</td>\n",
       "      <td>-0.028488</td>\n",
       "      <td>1.118007</td>\n",
       "      <td>1.150293</td>\n",
       "      <td>0.118381</td>\n",
       "      <td>0.753033</td>\n",
       "      <td>4.136079</td>\n",
       "      <td>0.409876</td>\n",
       "      <td>2.066245</td>\n",
       "      <td>3.610210</td>\n",
       "      <td>-2.139067</td>\n",
       "      <td>1.26094</td>\n",
       "      <td>2.330484</td>\n",
       "      <td>0.982421</td>\n",
       "      <td>0.182066</td>\n",
       "      <td>1.088451</td>\n",
       "      <td>-3.527752</td>\n",
       "      <td>0.701201</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>0.423965</td>\n",
       "      <td>-1.257774</td>\n",
       "      <td>-1.194013</td>\n",
       "      <td>-1.719062</td>\n",
       "      <td>1.360833</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>0.915523</td>\n",
       "      <td>-1.510224</td>\n",
       "      <td>-1.781693</td>\n",
       "      <td>-3.373969</td>\n",
       "      <td>0.88024</td>\n",
       "      <td>2.513074</td>\n",
       "      <td>0.389005</td>\n",
       "      <td>0.424964</td>\n",
       "      <td>1.992887</td>\n",
       "      <td>-2.616856</td>\n",
       "      <td>1.286974</td>\n",
       "      <td>0.561528</td>\n",
       "      <td>1.189605</td>\n",
       "      <td>-0.994041</td>\n",
       "      <td>0.099560</td>\n",
       "      <td>-2.485993</td>\n",
       "      <td>1.972286</td>\n",
       "      <td>2.08165</td>\n",
       "      <td>2.838853</td>\n",
       "      <td>0.499251</td>\n",
       "      <td>3.033732</td>\n",
       "      <td>1.513488</td>\n",
       "      <td>4.397532</td>\n",
       "      <td>1.266037</td>\n",
       "      <td>3.856384</td>\n",
       "      <td>1.013469</td>\n",
       "      <td>3</td>\n",
       "      <td>3.767339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.138531</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>-0.161518</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.195006</td>\n",
       "      <td>-0.143780</td>\n",
       "      <td>0.264749</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>2.683018</td>\n",
       "      <td>1.450991</td>\n",
       "      <td>0.342371</td>\n",
       "      <td>-0.074321</td>\n",
       "      <td>1.257761</td>\n",
       "      <td>0.632336</td>\n",
       "      <td>0.905204</td>\n",
       "      <td>0.575275</td>\n",
       "      <td>0.15616</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>2.550883</td>\n",
       "      <td>2.484082</td>\n",
       "      <td>0.460831</td>\n",
       "      <td>0.173712</td>\n",
       "      <td>2.502828</td>\n",
       "      <td>2.606440</td>\n",
       "      <td>2.731251</td>\n",
       "      <td>2.566561</td>\n",
       "      <td>0.386854</td>\n",
       "      <td>0.235952</td>\n",
       "      <td>-1.477905</td>\n",
       "      <td>-1.722451</td>\n",
       "      <td>0.37585</td>\n",
       "      <td>0.174779</td>\n",
       "      <td>-1.191981</td>\n",
       "      <td>-1.037629</td>\n",
       "      <td>-2.237275</td>\n",
       "      <td>-1.740456</td>\n",
       "      <td>0.326904</td>\n",
       "      <td>0.221809</td>\n",
       "      <td>-0.187586</td>\n",
       "      <td>-0.272907</td>\n",
       "      <td>0.870839</td>\n",
       "      <td>-1.256370</td>\n",
       "      <td>1.246881</td>\n",
       "      <td>-0.071239</td>\n",
       "      <td>2.085974</td>\n",
       "      <td>-0.864786</td>\n",
       "      <td>-1.794959</td>\n",
       "      <td>-1.706292</td>\n",
       "      <td>-1.503973</td>\n",
       "      <td>-0.903522</td>\n",
       "      <td>-1.493878</td>\n",
       "      <td>-0.916897</td>\n",
       "      <td>-2.874815</td>\n",
       "      <td>-2.452030</td>\n",
       "      <td>0.954714</td>\n",
       "      <td>0.545999</td>\n",
       "      <td>-1.572160</td>\n",
       "      <td>-1.265388</td>\n",
       "      <td>-0.402068</td>\n",
       "      <td>-1.185295</td>\n",
       "      <td>-0.986476</td>\n",
       "      <td>-1.794340</td>\n",
       "      <td>-1.995546</td>\n",
       "      <td>-4.252366</td>\n",
       "      <td>-1.793008</td>\n",
       "      <td>-1.181955</td>\n",
       "      <td>-2.116960</td>\n",
       "      <td>-2.030502</td>\n",
       "      <td>-2.810803</td>\n",
       "      <td>-3.467993</td>\n",
       "      <td>-2.050142</td>\n",
       "      <td>0.219379</td>\n",
       "      <td>0.410509</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.252536</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.170509</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>1.621499</td>\n",
       "      <td>-0.028488</td>\n",
       "      <td>1.697725</td>\n",
       "      <td>1.689662</td>\n",
       "      <td>0.016257</td>\n",
       "      <td>0.753033</td>\n",
       "      <td>0.464298</td>\n",
       "      <td>0.409876</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>0.187595</td>\n",
       "      <td>-2.788602</td>\n",
       "      <td>1.26094</td>\n",
       "      <td>4.345282</td>\n",
       "      <td>0.982421</td>\n",
       "      <td>2.737738</td>\n",
       "      <td>2.602937</td>\n",
       "      <td>-1.785502</td>\n",
       "      <td>0.701201</td>\n",
       "      <td>-0.172561</td>\n",
       "      <td>0.423965</td>\n",
       "      <td>-0.299516</td>\n",
       "      <td>-0.420021</td>\n",
       "      <td>-2.354611</td>\n",
       "      <td>1.360833</td>\n",
       "      <td>0.762192</td>\n",
       "      <td>0.915523</td>\n",
       "      <td>1.598620</td>\n",
       "      <td>0.623132</td>\n",
       "      <td>-1.742540</td>\n",
       "      <td>0.88024</td>\n",
       "      <td>-0.934675</td>\n",
       "      <td>0.389005</td>\n",
       "      <td>-0.373013</td>\n",
       "      <td>-1.213540</td>\n",
       "      <td>-3.677787</td>\n",
       "      <td>1.286974</td>\n",
       "      <td>2.684119</td>\n",
       "      <td>1.189605</td>\n",
       "      <td>2.861848</td>\n",
       "      <td>2.134804</td>\n",
       "      <td>-1.279284</td>\n",
       "      <td>1.972286</td>\n",
       "      <td>2.08165</td>\n",
       "      <td>0.344850</td>\n",
       "      <td>4.101145</td>\n",
       "      <td>0.614252</td>\n",
       "      <td>6.623456</td>\n",
       "      <td>0.800129</td>\n",
       "      <td>5.233243</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>3.926633</td>\n",
       "      <td>4</td>\n",
       "      <td>0.861351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date     weight    resp_1    resp_2    resp_3    resp_4      resp  \\\n",
       "0     0   0.000000  0.009916  0.014079  0.008773  0.001390  0.006270   \n",
       "1     0  16.673515 -0.002828 -0.003226 -0.007319 -0.011114 -0.009792   \n",
       "2     0   0.000000  0.025134  0.027607  0.033406  0.034380  0.023970   \n",
       "3     0   0.000000 -0.004730 -0.003273 -0.000461 -0.000476 -0.003200   \n",
       "4     0   0.138531  0.001252  0.002165 -0.001215 -0.006219 -0.002604   \n",
       "\n",
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0          1  -1.872746  -2.191242  -0.474163  -0.323046   0.014688   \n",
       "1         -1  -1.349537  -1.704709   0.068058   0.028432   0.193794   \n",
       "2         -1   0.812780  -0.256156   0.806463   0.400221  -0.614188   \n",
       "3         -1   1.174378   0.344640   0.066872   0.009357  -1.006373   \n",
       "4          1  -3.172026  -3.093182  -0.161518  -0.128149  -0.195006   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "0  -0.002484   0.264749     0.0127  -0.989982   -1.055090    0.342371   \n",
       "1   0.138212   0.264749     0.0127  -0.151877   -0.384952    0.342371   \n",
       "2  -0.354800   0.264749     0.0127   5.448261    2.668029    0.342371   \n",
       "3  -0.676458   0.264749     0.0127   4.508206    2.484260    0.342371   \n",
       "4  -0.143780   0.264749     0.0127   2.683018    1.450991    0.342371   \n",
       "\n",
       "   feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  \\\n",
       "0   -0.074321   -2.667671   -2.001475   -1.703595   -2.196892     0.15616   \n",
       "1   -0.074321    1.225838    0.789076    1.110580    1.102281     0.15616   \n",
       "2   -0.074321    3.836342    2.183258    3.902698    3.045431     0.15616   \n",
       "3   -0.074321    2.902176    1.799163    3.192700    2.848359     0.15616   \n",
       "4   -0.074321    1.257761    0.632336    0.905204    0.575275     0.15616   \n",
       "\n",
       "   feature_18  feature_19  feature_20  feature_21  feature_22  feature_23  \\\n",
       "0    0.033434    1.483295    1.307466    0.460831    0.173712    1.175200   \n",
       "1    0.033434   -0.590600   -0.625682    0.460831    0.173712   -0.543425   \n",
       "2    0.033434   -1.141082   -0.979962    0.460831    0.173712   -1.157585   \n",
       "3    0.033434   -1.401637   -1.428248    0.460831    0.173712   -1.421175   \n",
       "4    0.033434    2.550883    2.484082    0.460831    0.173712    2.502828   \n",
       "\n",
       "   feature_24  feature_25  feature_26  feature_27  feature_28  feature_29  \\\n",
       "0    0.967805    1.608410    1.319365    0.386854    0.235952   -0.515073   \n",
       "1   -0.547486   -0.706600   -0.667806    0.386854    0.235952    0.910558   \n",
       "2   -0.966803   -1.430973   -1.103432    0.386854    0.235952    5.131559   \n",
       "3   -1.487976   -1.756415   -1.647543    0.386854    0.235952    4.766182   \n",
       "4    2.606440    2.731251    2.566561    0.386854    0.235952   -1.477905   \n",
       "\n",
       "   feature_30  feature_31  feature_32  feature_33  feature_34  feature_35  \\\n",
       "0   -0.448988     0.37585    0.174779   -2.429812   -2.206423   -3.593120   \n",
       "1    0.914465     0.37585    0.174779    2.137454    2.080459    2.819291   \n",
       "2    4.314714     0.37585    0.174779    4.226341    3.173640    5.991513   \n",
       "3    4.528353     0.37585    0.174779    3.330068    2.778468    5.603940   \n",
       "4   -1.722451     0.37585    0.174779   -1.191981   -1.037629   -2.237275   \n",
       "\n",
       "   feature_36  feature_37  feature_38  feature_39  feature_40  feature_41  \\\n",
       "0   -2.868358    0.112697    0.053157   -0.539956   -0.692187    3.491282   \n",
       "1    2.483965   -0.086755   -0.082687    0.368431    0.469196    5.711996   \n",
       "2    4.142298   -0.167927   -0.124778    0.749326    0.715824   -0.039007   \n",
       "3    4.343171   -0.203161   -0.177835    0.642206    0.694692   -0.607811   \n",
       "4   -1.740456    0.326904    0.221809   -0.187586   -0.272907    0.870839   \n",
       "\n",
       "   feature_42  feature_43  feature_44  feature_45  feature_46  feature_47  \\\n",
       "0   -1.684889    1.337123   -0.328607    1.689207   -1.052243   -1.870885   \n",
       "1   -2.215132    0.796703   -1.140081    0.716617   -0.059431   -0.198920   \n",
       "2    0.186321    2.323887    0.162261    0.237987   -0.350221   -0.138033   \n",
       "3    2.718151    1.656999    0.192241   -0.622152   -0.024920    0.868425   \n",
       "4   -1.256370    1.246881   -0.071239    2.085974   -0.864786   -1.794959   \n",
       "\n",
       "   feature_48  feature_49  feature_50  feature_51  feature_52  feature_53  \\\n",
       "0   -1.789342   -1.574173   -1.120820   -0.571920   -1.093033    0.703515   \n",
       "1   -0.326697   -0.381770    1.435607    3.401393    2.486748   -2.014598   \n",
       "2   -0.516281   -0.703543   -0.556954   -0.816910   -0.455841    2.383226   \n",
       "3    0.414826    0.064472   -0.752845   -0.560471   -0.455841    2.979093   \n",
       "4   -1.706292   -1.503973   -0.903522   -1.493878   -0.916897   -2.874815   \n",
       "\n",
       "   feature_54  feature_55  feature_56  feature_57  feature_58  feature_59  \\\n",
       "0    5.936281    0.954714    3.315812    1.291338    2.468825    2.490069   \n",
       "1   -0.390588    0.954714   -0.027262   -1.886927   -1.706450   -0.888236   \n",
       "2    3.474416    0.954714    0.883247   -0.084247    0.622561    0.953619   \n",
       "3    0.770532    0.954714    0.574002    0.081969    0.840800    0.794274   \n",
       "4   -2.452030    0.954714    0.545999   -1.572160   -1.265388   -0.402068   \n",
       "\n",
       "   feature_60  feature_61  feature_62  feature_63  feature_64  feature_65  \\\n",
       "0   -1.148239   -0.961935   -2.263944   -2.158765   -5.012022   -2.006825   \n",
       "1   -1.138294   -0.954461   -1.350633   -1.459546   -4.564815   -2.651966   \n",
       "2   -1.128774   -0.946507   -1.470762   -1.594587   -4.346199   -2.276678   \n",
       "3   -1.127171   -0.945261   -1.416144   -1.531585   -4.322755   -3.377519   \n",
       "4   -1.185295   -0.986476   -1.794340   -1.995546   -4.252366   -1.793008   \n",
       "\n",
       "   feature_66  feature_67  feature_68  feature_69  feature_70  feature_71  \\\n",
       "0   -1.284090   -2.141697   -2.054935   -1.851203   -1.431184   -1.634481   \n",
       "1   -1.620014   -2.240625   -2.147273   -0.255224    3.202946   -0.535872   \n",
       "2   -1.417652   -2.166362   -2.077110    0.692339    0.467898   -0.297919   \n",
       "3   -2.010178   -2.190485   -2.099841    2.081633   -0.283574    0.938397   \n",
       "4   -1.181955   -2.116960   -2.030502   -2.810803   -3.467993   -2.050142   \n",
       "\n",
       "   feature_72  feature_73  feature_74  feature_75  feature_76  feature_77  \\\n",
       "0    0.219379   -0.373934    0.032995    0.559241    0.891368    0.271700   \n",
       "1    0.219379   -0.050948    0.032995    0.141089    0.058363    0.131190   \n",
       "2    0.219379   -0.463646    0.032995    0.129187   -0.426321    0.261728   \n",
       "3    0.219379    2.837515    0.032995    1.757084    2.730964    0.000788   \n",
       "4    0.219379    0.410509    0.032995    0.252536    0.420685    0.170509   \n",
       "\n",
       "   feature_78  feature_79  feature_80  feature_81  feature_82  feature_83  \\\n",
       "0    0.099493   -1.521125   -0.028488    3.045337    3.260512    0.683558   \n",
       "1    0.099493   -0.121239   -0.028488    0.677553    0.045842   -0.124616   \n",
       "2    0.099493   -0.456567   -0.028488    0.192444   -0.423503    0.309331   \n",
       "3    0.099493    1.206258   -0.028488    1.118007    1.150293    0.118381   \n",
       "4    0.099493    1.621499   -0.028488    1.697725    1.689662    0.016257   \n",
       "\n",
       "   feature_84  feature_85  feature_86  feature_87  feature_88  feature_89  \\\n",
       "0    0.753033   -0.109194    0.409876    0.488806    1.447504   -2.790902   \n",
       "1    0.753033   -0.007004    0.409876   -0.410491   -0.024323   -3.012654   \n",
       "2    0.753033    1.255440    0.409876    0.525988    0.934815   -2.881893   \n",
       "3    0.753033    4.136079    0.409876    2.066245    3.610210   -2.139067   \n",
       "4    0.753033    0.464298    0.409876   -0.032422    0.187595   -2.788602   \n",
       "\n",
       "   feature_90  feature_91  feature_92  feature_93  feature_94  feature_95  \\\n",
       "0     1.26094    1.158770    0.982421    3.754522    7.137163   -1.863069   \n",
       "1     1.26094    1.157671    0.982421    1.297679    1.281956   -2.427595   \n",
       "2     1.26094    2.420089    0.982421    0.800962    1.143663   -3.214578   \n",
       "3     1.26094    2.330484    0.982421    0.182066    1.088451   -3.527752   \n",
       "4     1.26094    4.345282    0.982421    2.737738    2.602937   -1.785502   \n",
       "\n",
       "   feature_96  feature_97  feature_98  feature_99  feature_100  feature_101  \\\n",
       "0    0.701201    0.434466    0.423965   -0.292035     0.317003    -2.605820   \n",
       "1    0.701201    0.024913    0.423965   -0.413607    -0.073672    -2.434546   \n",
       "2    0.701201    1.585939    0.423965    0.193996     0.953114    -2.674838   \n",
       "3    0.701201   -1.338859    0.423965   -1.257774    -1.194013    -1.719062   \n",
       "4    0.701201   -0.172561    0.423965   -0.299516    -0.420021    -2.354611   \n",
       "\n",
       "   feature_102  feature_103  feature_104  feature_105  feature_106  \\\n",
       "0     1.360833     2.896986     0.915523     1.485813     4.147254   \n",
       "1     1.360833     0.949879     0.915523     0.724655     1.622137   \n",
       "2     1.360833     2.200085     0.915523     0.537175     2.156228   \n",
       "3     1.360833    -0.940190     0.915523    -1.510224    -1.781693   \n",
       "4     1.360833     0.762192     0.915523     1.598620     0.623132   \n",
       "\n",
       "   feature_107  feature_108  feature_109  feature_110  feature_111  \\\n",
       "0    -2.238831      0.88024    -0.892724     0.389005    -0.156332   \n",
       "1    -2.209020      0.88024    -1.332492     0.389005    -0.586619   \n",
       "2    -3.568648      0.88024     1.193823     0.389005     0.097345   \n",
       "3    -3.373969      0.88024     2.513074     0.389005     0.424964   \n",
       "4    -1.742540      0.88024    -0.934675     0.389005    -0.373013   \n",
       "\n",
       "   feature_112  feature_113  feature_114  feature_115  feature_116  \\\n",
       "0     0.622816    -3.921523     1.286974     2.561593     1.189605   \n",
       "1    -1.040491    -3.946097     1.286974     0.983440     1.189605   \n",
       "2     0.796214    -4.090058     1.286974     2.548596     1.189605   \n",
       "3     1.992887    -2.616856     1.286974     0.561528     1.189605   \n",
       "4    -1.213540    -3.677787     1.286974     2.684119     1.189605   \n",
       "\n",
       "   feature_117  feature_118  feature_119  feature_120  feature_121  \\\n",
       "0     3.457757     6.649580    -1.472686     1.972286      2.08165   \n",
       "1     1.357907     1.612348    -1.664544     1.972286      2.08165   \n",
       "2     0.882588     1.817895    -2.432424     1.972286      2.08165   \n",
       "3    -0.994041     0.099560    -2.485993     1.972286      2.08165   \n",
       "4     2.861848     2.134804    -1.279284     1.972286      2.08165   \n",
       "\n",
       "   feature_122  feature_123  feature_124  feature_125  feature_126  \\\n",
       "0     1.168391     8.313583     1.782433    14.018213     2.653056   \n",
       "1    -1.178850     1.777472    -0.915458     2.831612    -1.417010   \n",
       "2     6.115747     9.667908     5.542871    11.671595     7.281757   \n",
       "3     2.838853     0.499251     3.033732     1.513488     4.397532   \n",
       "4     0.344850     4.101145     0.614252     6.623456     0.800129   \n",
       "\n",
       "   feature_127  feature_128  feature_129  ts_id  stock_id  \n",
       "0    12.600292     2.301488    11.445807      0  3.143516  \n",
       "1     2.297459    -1.304614     1.898684      1  4.293568  \n",
       "2    10.060014     6.638248     9.427299      2  2.471201  \n",
       "3     1.266037     3.856384     1.013469      3  3.767339  \n",
       "4     5.233243     0.362636     3.926633      4  0.861351  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prescription-graphic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129']\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for item in train_dev.columns:\n",
    "#     if 'feature' in item or 'weight' in item:\n",
    "    if 'feature' in item:\n",
    "        features.append(item)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "further-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mean = train_dev.loc[:, features].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "loving-intermediate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dev.fillna(train_dev.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "biological-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev[\"stock_id\"] = train_dev['feature_41'] + train_dev['feature_42'] + train_dev['feature_43'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "gentle-explosion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.872746</td>\n",
       "      <td>-2.191242</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168391</td>\n",
       "      <td>8.313583</td>\n",
       "      <td>1.782433</td>\n",
       "      <td>14.018213</td>\n",
       "      <td>2.653056</td>\n",
       "      <td>12.600292</td>\n",
       "      <td>2.301488</td>\n",
       "      <td>11.445807</td>\n",
       "      <td>0</td>\n",
       "      <td>3.143516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16.673515</td>\n",
       "      <td>-0.002828</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>-0.007319</td>\n",
       "      <td>-0.011114</td>\n",
       "      <td>-0.009792</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.349537</td>\n",
       "      <td>-1.704709</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.178850</td>\n",
       "      <td>1.777472</td>\n",
       "      <td>-0.915458</td>\n",
       "      <td>2.831612</td>\n",
       "      <td>-1.417010</td>\n",
       "      <td>2.297459</td>\n",
       "      <td>-1.304614</td>\n",
       "      <td>1.898684</td>\n",
       "      <td>1</td>\n",
       "      <td>4.293568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.027607</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>0.034380</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.812780</td>\n",
       "      <td>-0.256156</td>\n",
       "      <td>...</td>\n",
       "      <td>6.115747</td>\n",
       "      <td>9.667908</td>\n",
       "      <td>5.542871</td>\n",
       "      <td>11.671595</td>\n",
       "      <td>7.281757</td>\n",
       "      <td>10.060014</td>\n",
       "      <td>6.638248</td>\n",
       "      <td>9.427299</td>\n",
       "      <td>2</td>\n",
       "      <td>2.471201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004730</td>\n",
       "      <td>-0.003273</td>\n",
       "      <td>-0.000461</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.174378</td>\n",
       "      <td>0.344640</td>\n",
       "      <td>...</td>\n",
       "      <td>2.838853</td>\n",
       "      <td>0.499251</td>\n",
       "      <td>3.033732</td>\n",
       "      <td>1.513488</td>\n",
       "      <td>4.397532</td>\n",
       "      <td>1.266037</td>\n",
       "      <td>3.856384</td>\n",
       "      <td>1.013469</td>\n",
       "      <td>3</td>\n",
       "      <td>3.767339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.138531</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344850</td>\n",
       "      <td>4.101145</td>\n",
       "      <td>0.614252</td>\n",
       "      <td>6.623456</td>\n",
       "      <td>0.800129</td>\n",
       "      <td>5.233243</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>3.926633</td>\n",
       "      <td>4</td>\n",
       "      <td>0.861351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date     weight    resp_1    resp_2    resp_3    resp_4      resp  \\\n",
       "0     0   0.000000  0.009916  0.014079  0.008773  0.001390  0.006270   \n",
       "1     0  16.673515 -0.002828 -0.003226 -0.007319 -0.011114 -0.009792   \n",
       "2     0   0.000000  0.025134  0.027607  0.033406  0.034380  0.023970   \n",
       "3     0   0.000000 -0.004730 -0.003273 -0.000461 -0.000476 -0.003200   \n",
       "4     0   0.138531  0.001252  0.002165 -0.001215 -0.006219 -0.002604   \n",
       "\n",
       "   feature_0  feature_1  feature_2  ...  feature_122  feature_123  \\\n",
       "0          1  -1.872746  -2.191242  ...     1.168391     8.313583   \n",
       "1         -1  -1.349537  -1.704709  ...    -1.178850     1.777472   \n",
       "2         -1   0.812780  -0.256156  ...     6.115747     9.667908   \n",
       "3         -1   1.174378   0.344640  ...     2.838853     0.499251   \n",
       "4          1  -3.172026  -3.093182  ...     0.344850     4.101145   \n",
       "\n",
       "   feature_124  feature_125  feature_126  feature_127  feature_128  \\\n",
       "0     1.782433    14.018213     2.653056    12.600292     2.301488   \n",
       "1    -0.915458     2.831612    -1.417010     2.297459    -1.304614   \n",
       "2     5.542871    11.671595     7.281757    10.060014     6.638248   \n",
       "3     3.033732     1.513488     4.397532     1.266037     3.856384   \n",
       "4     0.614252     6.623456     0.800129     5.233243     0.362636   \n",
       "\n",
       "   feature_129  ts_id  stock_id  \n",
       "0    11.445807      0  3.143516  \n",
       "1     1.898684      1  4.293568  \n",
       "2     9.427299      2  2.471201  \n",
       "3     1.013469      3  3.767339  \n",
       "4     3.926633      4  0.861351  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "recognized-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dev = train_dev.loc[:, features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "quantitative-mercury",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a easy NN model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Create a easy NN model...')\n",
    "\n",
    "HIDDEN_LAYER = [160, 160, 160]\n",
    "#TARGET_NUM = 5   # 优化那5个resp\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(X_train_dev.shape[1], ))\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(input)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "for units in HIDDEN_LAYER:\n",
    "    x = tf.keras.layers.Dense(units)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)  # 除了ReLU还可以试试别的，后面可以做一个多模型融合\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "output = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=input, outputs=output)\n",
    "model.compile(\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics   = tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    loss      = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n",
    ")\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train NN...')\n",
    "\n",
    "history = model.fit(\n",
    "    x = X_train, \n",
    "    y = y_train, \n",
    "    epochs=200, \n",
    "    batch_size=4096, \n",
    "#     validation_data=(X_valid, y_valid),\n",
    "#     callbacks = EarlyStopping(monitor='roc_auc', patience=20, verbose=2, mode='max')\n",
    ")\n",
    "models = []\n",
    "models.append(model)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-duration",
   "metadata": {},
   "source": [
    "### Data Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frozen-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/train.csv\"\n",
    "train_path = \"../data/training.csv\"\n",
    "test_path = \"../data/testing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "behind-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path) as f:\n",
    "    columns = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-mexico",
   "metadata": {},
   "source": [
    "#### split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distributed-activity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def split_data(file_path, train_file, test_file):\n",
    "    \"\"\"\n",
    "    split train and test data\n",
    "    for date<400, write to train data file,\n",
    "    for date>=400, write to test data file.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        columns = f.readline()\n",
    "    \n",
    "    if os.path.exists(train_file):\n",
    "        os.remove(train_file)\n",
    "    if os.path.exists(test_file):\n",
    "        os.remove(test_file)  \n",
    "    print(\"splitting data...\")\n",
    "    with open(train_file, \"a\") as train:\n",
    "        train.write(columns)\n",
    "    with open(test_file, \"a\") as test:\n",
    "        test.write(columns)\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            date = line.split(\",\")[0]\n",
    "            if not date.isdigit():\n",
    "                continue\n",
    "            if int(date) < 400:\n",
    "                with open(train_file, \"a\") as train:\n",
    "                    train.write(line)\n",
    "            else:\n",
    "                with open(test_file, \"a\") as test:\n",
    "                    test.write(line)\n",
    "\n",
    "split_data(file_path, train_path, test_path)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "shared-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date,weight,resp_1,resp_2,resp_3,resp_4,resp,feature_0,feature_1,feature_2,feature_3,feature_4,feature_5,feature_6,feature_7,feature_8,feature_9,feature_10,feature_11,feature_12,feature_13,feature_14,feature_15,feature_16,feature_17,feature_18,feature_19,feature_20,feature_21,feature_22,feature_23,feature_24,feature_25,feature_26,feature_27,feature_28,feature_29,feature_30,feature_31,feature_32,feature_33,feature_34,feature_35,feature_36,feature_37,feature_38,feature_39,feature_40,feature_41,feature_42,feature_43,feature_44,feature_45,feature_46,feature_47,feature_48,feature_49,feature_50,feature_51,feature_52,feature_53,feature_54,feature_55,feature_56,feature_57,feature_58,feature_59,feature_60,feature_61,feature_62,feature_63,feature_64,feature_65,feature_66,feature_67,feature_68,feature_69,feature_70,feature_71,feature_72,feature_73,feature_74,feature_75,feature_76,feature_77,feature_78,feature_79,feature_80,feature_81,feature_82,feature_83,feature_84,feature_85,feature_86,feature_87,feature_88,feature_89,feature_90,feature_91,feature_92,feature_93,feature_94,feature_95,feature_96,feature_97,feature_98,feature_99,feature_100,feature_101,feature_102,feature_103,feature_104,feature_105,feature_106,feature_107,feature_108,feature_109,feature_110,feature_111,feature_112,feature_113,feature_114,feature_115,feature_116,feature_117,feature_118,feature_119,feature_120,feature_121,feature_122,feature_123,feature_124,feature_125,feature_126,feature_127,feature_128,feature_129,ts_id\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(train_path) as t:\n",
    "    print(t.readline())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sitting-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = tf.data.experimental.make_csv_dataset(\n",
    "    train_path, batch_size=4,shuffle=False,\n",
    "    label_name=\"resp\")\n",
    "test_batches = tf.data.experimental.make_csv_dataset(\n",
    "    test_path, batch_size=4,shuffle=False,\n",
    "    label_name=\"resp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-progressive",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tamil-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(train_batches))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "outstanding-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('date',\n",
       "              <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0])>),\n",
       "             ('weight',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.      , 16.673515,  0.      ,  0.      ], dtype=float32)>),\n",
       "             ('resp_1',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.00991599, -0.00282823,  0.02513386, -0.00473014], dtype=float32)>),\n",
       "             ('resp_2',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.01407858, -0.00322632,  0.02760739, -0.00327283], dtype=float32)>),\n",
       "             ('resp_3',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.00877305, -0.00731949,  0.03340572, -0.00046082], dtype=float32)>),\n",
       "             ('resp_4',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.00139037, -0.01111406,  0.03437974, -0.0004762 ], dtype=float32)>),\n",
       "             ('feature_0',\n",
       "              <tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 1, -1, -1, -1])>),\n",
       "             ('feature_1',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.8727463 , -1.349537  ,  0.81278044,  1.1743785 ], dtype=float32)>),\n",
       "             ('feature_2',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.1912425 , -1.7047089 , -0.25615585,  0.34464008], dtype=float32)>),\n",
       "             ('feature_3',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.47416273,  0.06805764,  0.80646265,  0.06687216], dtype=float32)>),\n",
       "             ('feature_4',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.32304582,  0.02843231,  0.4002206 ,  0.00935715], dtype=float32)>),\n",
       "             ('feature_5',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.01468829,  0.193794  , -0.6141878 , -1.0063726 ], dtype=float32)>),\n",
       "             ('feature_6',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.00248358,  0.13821208, -0.35480005, -0.6764579 ], dtype=float32)>),\n",
       "             ('feature_7',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_8',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_9',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.9899824 , -0.15187724,  5.448261  ,  4.508206  ], dtype=float32)>),\n",
       "             ('feature_10',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.0550903 , -0.38495228,  2.6680288 ,  2.48426   ], dtype=float32)>),\n",
       "             ('feature_11',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_12',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_13',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.6676705,  1.2258382,  3.8363416,  2.9021764], dtype=float32)>),\n",
       "             ('feature_14',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.0014746,  0.7890762,  2.1832576,  1.799163 ], dtype=float32)>),\n",
       "             ('feature_15',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.7035946,  1.1105801,  3.9026978,  3.1926997], dtype=float32)>),\n",
       "             ('feature_16',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.1968923,  1.102281 ,  3.0454307,  2.8483586], dtype=float32)>),\n",
       "             ('feature_17',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_18',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_19',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.483295 , -0.5905997, -1.1410816, -1.4016367], dtype=float32)>),\n",
       "             ('feature_20',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.3074659 , -0.62568206, -0.97996235, -1.4282483 ], dtype=float32)>),\n",
       "             ('feature_21',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_22',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_23',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.1752   , -0.5434252, -1.1575847, -1.4211752], dtype=float32)>),\n",
       "             ('feature_24',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.9678054, -0.5474857, -0.9668032, -1.4879756], dtype=float32)>),\n",
       "             ('feature_25',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.6084098 , -0.70660037, -1.4309726 , -1.7564151 ], dtype=float32)>),\n",
       "             ('feature_26',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.3193651, -0.6678061, -1.1034316, -1.647543 ], dtype=float32)>),\n",
       "             ('feature_27',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_28',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_29',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.5150725,  0.9105579,  5.131559 ,  4.7661815], dtype=float32)>),\n",
       "             ('feature_30',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.4489878 ,  0.91446495,  4.3147144 ,  4.528353  ], dtype=float32)>),\n",
       "             ('feature_31',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_32',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_33',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.4298124,  2.137454 ,  4.2263412,  3.330068 ], dtype=float32)>),\n",
       "             ('feature_34',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.2064233,  2.080459 ,  3.1736403,  2.7784684], dtype=float32)>),\n",
       "             ('feature_35',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-3.59312  ,  2.8192909,  5.9915133,  5.60394  ], dtype=float32)>),\n",
       "             ('feature_36',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.8683584,  2.483965 ,  4.1422977,  4.343171 ], dtype=float32)>),\n",
       "             ('feature_37',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.11269682, -0.08675475, -0.16792715, -0.20316108], dtype=float32)>),\n",
       "             ('feature_38',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.05315671, -0.08268737, -0.12477773, -0.17783503], dtype=float32)>),\n",
       "             ('feature_39',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.53995574,  0.36843082,  0.7493258 ,  0.6422065 ], dtype=float32)>),\n",
       "             ('feature_40',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.69218665,  0.46919593,  0.7158244 ,  0.6946919 ], dtype=float32)>),\n",
       "             ('feature_41',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.4912822 ,  5.7119956 , -0.03900658, -0.6078106 ], dtype=float32)>),\n",
       "             ('feature_42',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.6848891 , -2.2151315 ,  0.18632062,  2.7181509 ], dtype=float32)>),\n",
       "             ('feature_43',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([1.3371226 , 0.79670346, 2.3238873 , 1.656999  ], dtype=float32)>),\n",
       "             ('feature_44',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.3286072 , -1.1400806 ,  0.16226144,  0.19224124], dtype=float32)>),\n",
       "             ('feature_45',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.6892067 ,  0.7166166 ,  0.23798735, -0.62215227], dtype=float32)>),\n",
       "             ('feature_46',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.0522431 , -0.05943131, -0.35022137, -0.02491983], dtype=float32)>),\n",
       "             ('feature_47',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.8708851 , -0.19891977, -0.13803296,  0.8684251 ], dtype=float32)>),\n",
       "             ('feature_48',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.7893417 , -0.326697  , -0.5162811 ,  0.41482642], dtype=float32)>),\n",
       "             ('feature_49',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.5741733 , -0.3817701 , -0.7035434 ,  0.06447222], dtype=float32)>),\n",
       "             ('feature_50',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.1208204,  1.4356073, -0.5569541, -0.7528454], dtype=float32)>),\n",
       "             ('feature_51',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.57192004,  3.401393  , -0.81690973, -0.5604712 ], dtype=float32)>),\n",
       "             ('feature_52',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.0930334 ,  2.4867475 , -0.45584133, -0.45584133], dtype=float32)>),\n",
       "             ('feature_53',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.7035151, -2.014598 ,  2.3832264,  2.9790928], dtype=float32)>),\n",
       "             ('feature_54',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 5.9362807 , -0.39058793,  3.4744165 ,  0.7705322 ], dtype=float32)>),\n",
       "             ('feature_55',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_56',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.315812  , -0.02726198,  0.88324666,  0.57400215], dtype=float32)>),\n",
       "             ('feature_57',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.2913378 , -1.8869272 , -0.08424736,  0.0819693 ], dtype=float32)>),\n",
       "             ('feature_58',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.4688246 , -1.7064499 ,  0.62256056,  0.8408    ], dtype=float32)>),\n",
       "             ('feature_59',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.4900692, -0.8882357,  0.9536187,  0.7942739], dtype=float32)>),\n",
       "             ('feature_60',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.1482393, -1.138294 , -1.1287743, -1.1271708], dtype=float32)>),\n",
       "             ('feature_61',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.96193457, -0.95446134, -0.9465075 , -0.9452608 ], dtype=float32)>),\n",
       "             ('feature_62',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.2639444, -1.3506329, -1.4707624, -1.4161435], dtype=float32)>),\n",
       "             ('feature_63',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.1587648, -1.4595462, -1.5945868, -1.5315847], dtype=float32)>),\n",
       "             ('feature_64',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5.0120225, -4.5648146, -4.346199 , -4.3227544], dtype=float32)>),\n",
       "             ('feature_65',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.0068252, -2.6519659, -2.2766778, -3.3775191], dtype=float32)>),\n",
       "             ('feature_66',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.2840902, -1.6200144, -1.4176515, -2.010178 ], dtype=float32)>),\n",
       "             ('feature_67',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.1416967, -2.2406247, -2.166362 , -2.1904848], dtype=float32)>),\n",
       "             ('feature_68',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.054935 , -2.1472728, -2.0771098, -2.0998416], dtype=float32)>),\n",
       "             ('feature_69',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.8512031 , -0.25522363,  0.69233906,  2.0816333 ], dtype=float32)>),\n",
       "             ('feature_70',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.431184  ,  3.2029462 ,  0.46789846, -0.28357357], dtype=float32)>),\n",
       "             ('feature_71',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.6344812 , -0.53587204, -0.29791936,  0.9383969 ], dtype=float32)>),\n",
       "             ('feature_72',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_73',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.3739341 , -0.05094803, -0.46364585,  2.8375146 ], dtype=float32)>),\n",
       "             ('feature_74',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_75',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.559241  , 0.14108853, 0.12918718, 1.7570841 ], dtype=float32)>),\n",
       "             ('feature_76',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.89136803,  0.05836299, -0.42632145,  2.7309642 ], dtype=float32)>),\n",
       "             ('feature_77',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.2717    , 0.13118976, 0.26172796, 0.00078756], dtype=float32)>),\n",
       "             ('feature_78',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_79',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.5211253 , -0.12123876, -0.45656687,  1.2062578 ], dtype=float32)>),\n",
       "             ('feature_80',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_81',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([3.0453365 , 0.6775533 , 0.19244395, 1.1180074 ], dtype=float32)>),\n",
       "             ('feature_82',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.2605119 ,  0.04584193, -0.42350304,  1.1502926 ], dtype=float32)>),\n",
       "             ('feature_83',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.68355775, -0.1246164 ,  0.30933106,  0.11838095], dtype=float32)>),\n",
       "             ('feature_84',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_85',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.10919381, -0.00700393,  1.2554404 ,  4.136079  ], dtype=float32)>),\n",
       "             ('feature_86',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_87',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.4888065, -0.4104912,  0.525988 ,  2.0662453], dtype=float32)>),\n",
       "             ('feature_88',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.4475038 , -0.02432276,  0.93481463,  3.6102102 ], dtype=float32)>),\n",
       "             ('feature_89',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.790902 , -3.0126538, -2.8818924, -2.1390665], dtype=float32)>),\n",
       "             ('feature_90',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_91',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([1.1587703, 1.1576705, 2.4200885, 2.3304844], dtype=float32)>),\n",
       "             ('feature_92',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_93',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([3.754522  , 1.2976786 , 0.8009622 , 0.18206635], dtype=float32)>),\n",
       "             ('feature_94',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([7.137163 , 1.2819561, 1.1436635, 1.0884511], dtype=float32)>),\n",
       "             ('feature_95',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.8630686, -2.4275947, -3.214578 , -3.527752 ], dtype=float32)>),\n",
       "             ('feature_96',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_97',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.43446568,  0.02491278,  1.5859394 , -1.3388586 ], dtype=float32)>),\n",
       "             ('feature_98',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_99',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.29203495, -0.41360697,  0.1939958 , -1.2577736 ], dtype=float32)>),\n",
       "             ('feature_100',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.3170029 , -0.07367184,  0.9531143 , -1.1940129 ], dtype=float32)>),\n",
       "             ('feature_101',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.6058202, -2.4345462, -2.6748385, -1.7190617], dtype=float32)>),\n",
       "             ('feature_102',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_103',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.8969858 ,  0.94987905,  2.2000854 , -0.9401897 ], dtype=float32)>),\n",
       "             ('feature_104',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_105',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.485813  ,  0.72465545,  0.53717494, -1.5102236 ], dtype=float32)>),\n",
       "             ('feature_106',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 4.147254 ,  1.622137 ,  2.1562276, -1.7816925], dtype=float32)>),\n",
       "             ('feature_107',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-2.2388313, -2.2090197, -3.568648 , -3.3739686], dtype=float32)>),\n",
       "             ('feature_108',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_109',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.8927239, -1.3324925,  1.1938231,  2.5130737], dtype=float32)>),\n",
       "             ('feature_110',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_111',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.1563317 , -0.58661896,  0.09734456,  0.42496365], dtype=float32)>),\n",
       "             ('feature_112',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.62281597, -1.0404913 ,  0.796214  ,  1.992887  ], dtype=float32)>),\n",
       "             ('feature_113',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-3.9215233, -3.9460974, -4.090058 , -2.616856 ], dtype=float32)>),\n",
       "             ('feature_114',\n",
       "              <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'', b'', b'', b''], dtype=object)>),\n",
       "             ('feature_115',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([2.5615933 , 0.98343974, 2.5485957 , 0.5615284 ], dtype=float32)>),\n",
       "             ('feature_116',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_117',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.4577568 ,  1.3579072 ,  0.8825883 , -0.99404055], dtype=float32)>),\n",
       "             ('feature_118',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([6.64958   , 1.612348  , 1.817895  , 0.09956049], dtype=float32)>),\n",
       "             ('feature_119',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.4726858, -1.6645439, -2.4324236, -2.4859934], dtype=float32)>),\n",
       "             ('feature_120',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_121',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>),\n",
       "             ('feature_122',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.1683913, -1.1788498,  6.1157475,  2.8388534], dtype=float32)>),\n",
       "             ('feature_123',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([8.313582 , 1.777472 , 9.667908 , 0.4992508], dtype=float32)>),\n",
       "             ('feature_124',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.7824326, -0.9154585,  5.542871 ,  3.0337315], dtype=float32)>),\n",
       "             ('feature_125',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([14.018213 ,  2.831612 , 11.671595 ,  1.5134882], dtype=float32)>),\n",
       "             ('feature_126',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.6530561, -1.4170104,  7.2817574,  4.3975325], dtype=float32)>),\n",
       "             ('feature_127',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([12.600291 ,  2.2974591, 10.060014 ,  1.2660372], dtype=float32)>),\n",
       "             ('feature_128',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.3014882, -1.3046144,  6.638248 ,  3.8563836], dtype=float32)>),\n",
       "             ('feature_129',\n",
       "              <tf.Tensor: shape=(4,), dtype=float32, numpy=array([11.4458065,  1.8986844,  9.4272995,  1.0134695], dtype=float32)>),\n",
       "             ('ts_id',\n",
       "              <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3])>)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True)\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "composed-carry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0 0 1], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_batches)\n",
    "\n",
    "print((next(it)[0][\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "standing-passport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'resp': [ 0.01519238  0.01568617  0.00022552 -0.00087303]\n",
      "features:\n",
      "  'date'              : [1 0 0 0]\n",
      "  'weight'            : [0.47025874 0.         0.         1.4328338 ]\n",
      "  'resp_1'            : [-0.00471086  0.00854328  0.00094773  0.00151781]\n",
      "  'resp_2'            : [-0.00250607  0.00904607  0.00094773  0.00172566]\n",
      "  'resp_3'            : [ 0.00447743  0.01842721 -0.00117745  0.00153187]\n",
      "  'resp_4'            : [ 0.02102417  0.02528852 -0.00272663 -0.0013315 ]\n",
      "  'feature_0'         : [1 1 1 1]\n",
      "  'feature_1'         : [-1.663064    0.8587217   0.38455975  0.72780573]\n",
      "  'feature_2'         : [-2.1830888  -0.42880377 -0.35049912 -0.14469783]\n",
      "  'feature_3'         : [0.49478307 0.5371123  0.25024635 0.27679375]\n",
      "  'feature_4'         : [0.228487   0.21077642 0.12087049 0.12522024]\n",
      "  'feature_5'         : [-0.23335774  0.58595365  0.36532754  0.5616513 ]\n",
      "  'feature_6'         : [-0.14445534  0.26930267  0.2196222   0.3280783 ]\n",
      "  'feature_7'         : [b'-2.201458714763928' b'' b'0.3070672657904248' b'-0.08911908054397144']\n",
      "  'feature_8'         : [b'-1.937750358664476' b'' b'0.08184165328412132' b'-0.2452178889749071']\n",
      "  'feature_9'         : [-0.8716046   3.3097856  -0.19507986  0.53458744]\n",
      "  'feature_10'        : [-1.1359307   1.0397485  -0.63817334 -0.24866256]\n",
      "  'feature_11'        : [-1.4383441   2.787507   -0.24214768 -0.08945036]\n",
      "  'feature_12'        : [-1.8954027  2.5743053 -0.6415901 -0.5024555]\n",
      "  'feature_13'        : [-1.5570092   2.8252978  -0.87738097 -0.16397724]\n",
      "  'feature_14'        : [-1.2313318   1.209905   -0.90624326 -0.55309194]\n",
      "  'feature_15'        : [-1.7402478   2.8149884  -0.58981395 -0.15016797]\n",
      "  'feature_16'        : [-2.203533   1.687065  -1.3155272 -0.923791 ]\n",
      "  'feature_17'        : [b'0.08061822242761257' b'' b'0.6354448923177531' b'0.7320801591015522']\n",
      "  'feature_18'        : [b'0.0016368071271748' b'' b'0.509060645626445' b'0.5657289104939047']\n",
      "  'feature_19'        : [0.4295589  3.9541473  0.80248106 1.1545376 ]\n",
      "  'feature_20'        : [0.13960718 2.804661   0.5412177  0.8166275 ]\n",
      "  'feature_21'        : [0.25904402 4.693562   0.518046   0.69497275]\n",
      "  'feature_22'        : [0.03561285 2.1665788  0.15866357 0.21123238]\n",
      "  'feature_23'        : [0.29154092 3.170116   0.6917293  0.9006202 ]\n",
      "  'feature_24'        : [0.06480779 2.4040463  0.41547796 0.55145645]\n",
      "  'feature_25'        : [0.45136818 4.15309    0.9829133  1.2527165 ]\n",
      "  'feature_26'        : [0.11923227 2.8989801  0.6204928  0.79965645]\n",
      "  'feature_27'        : [b'-2.05336254313805' b'' b'-0.09729783795609333' b'-0.09177464832321397']\n",
      "  'feature_28'        : [b'-2.27961112657526' b'' b'-0.07064000746369985' b'-0.05603067113582789']\n",
      "  'feature_29'        : [-0.3863299  -1.3650097  -0.29948944 -0.576297  ]\n",
      "  'feature_30'        : [-0.2629083  -1.1182376  -0.23570752 -0.4575881 ]\n",
      "  'feature_31'        : [-1.3576314  -0.60741943 -0.2151604  -0.2040321 ]\n",
      "  'feature_32'        : [-1.8718439  -0.6127876  -0.191741   -0.15696661]\n",
      "  'feature_33'        : [-1.738263   -1.1013811  -0.15625891 -0.4024275 ]\n",
      "  'feature_34'        : [-1.3619564  -0.64297104 -0.0736878  -0.20444086]\n",
      "  'feature_35'        : [-1.3106836  -2.0698876  -0.36632404 -0.8184331 ]\n",
      "  'feature_36'        : [-0.66966647 -1.1103059  -0.18721382 -0.41223896]\n",
      "  'feature_37'        : [0.55227053 1.2471768  0.6337749  1.7941135 ]\n",
      "  'feature_38'        : [0.3137683  0.7262973  0.44146454 1.3991531 ]\n",
      "  'feature_39'        : [-1.6298223  -0.10122628 -0.4224694  -0.02813623]\n",
      "  'feature_40'        : [-1.8336577  -0.08998436 -0.5335506  -0.01273446]\n",
      "  'feature_41'        : [ 1.6598264 -2.1634514  1.7337184 -1.2373341]\n",
      "  'feature_42'        : [-0.84606504  3.967654   -1.0065033   1.5705163 ]\n",
      "  'feature_43'        : [2.2314692 2.9477372 1.5604672 1.839233 ]\n",
      "  'feature_44'        : [-0.69425595 -2.8908577   0.57827884 -2.1089299 ]\n",
      "  'feature_45'        : [ 1.5146377  -1.4441944   2.1257153  -0.94966614]\n",
      "  'feature_46'        : [-0.458788   0.5659429  1.7274071  2.9281926]\n",
      "  'feature_47'        : [-1.0866135  2.4030674  1.3359135  4.5712132]\n",
      "  'feature_48'        : [-1.2221841  1.4888895  0.8664185  4.9721212]\n",
      "  'feature_49'        : [-1.1999356  0.6034451  0.4382431  3.6838782]\n",
      "  'feature_50'        : [-0.30215684 -0.35615435  4.3055205   2.4932854 ]\n",
      "  'feature_51'        : [-0.46059197 -1.12667     2.7501488   0.40878153]\n",
      "  'feature_52'        : [-1.2527748  -1.623502   -0.01495607  0.55564713]\n",
      "  'feature_53'        : [-0.74421626  5.4319434  -0.8980373  -0.3949228 ]\n",
      "  'feature_54'        : [ 0.16469796  1.6000533   0.12114705 -1.9510734 ]\n",
      "  'feature_55'        : [-0.89666384 -0.8881149  -0.97931594 -0.1454875 ]\n",
      "  'feature_56'        : [-0.4694488  -0.47757632 -0.26672974  0.33218467]\n",
      "  'feature_57'        : [-1.2535684  -0.48411587 -1.1909336  -0.511958  ]\n",
      "  'feature_58'        : [-2.0734754  -0.28253603 -1.9397368  -0.96802163]\n",
      "  'feature_59'        : [-1.2176447  -0.74040705 -0.6482683  -0.14633287]\n",
      "  'feature_60'        : [-3.2535322  -0.5655559  -0.34035823 -1.6675633 ]\n",
      "  'feature_61'        : [-2.666864   -0.4491985  -0.29372907 -1.3037822 ]\n",
      "  'feature_62'        : [ 0.8343742  -0.27284864  2.481703    5.5122466 ]\n",
      "  'feature_63'        : [ 0.8674543  -0.29671073  2.5611563   5.6566377 ]\n",
      "  'feature_64'        : [-0.3292254 -2.572309   5.162911   2.7101946]\n",
      "  'feature_65'        : [ 0.95396537 -1.2657764   5.8707285   4.048572  ]\n",
      "  'feature_66'        : [ 0.7924362 -0.8981518  4.8554735  3.4678497]\n",
      "  'feature_67'        : [ 0.26037633 -4.294538    3.8805552  -3.168449  ]\n",
      "  'feature_68'        : [ 0.26131406 -4.0755825   3.8200252  -2.987439  ]\n",
      "  'feature_69'        : [-1.0859882  2.4710207 -0.1393867  1.3355458]\n",
      "  'feature_70'        : [-0.9102232  -0.72692984  1.465491   -0.1858025 ]\n",
      "  'feature_71'        : [-1.4203589   0.6739036  -0.6832987   0.34743717]\n",
      "  'feature_72'        : [b'0.1885888096591004' b'' b'-0.4927237406746618' b'0.9803604417783628']\n",
      "  'feature_73'        : [-0.19655064 -2.6340158  -1.2316797  -2.9319782 ]\n",
      "  'feature_74'        : [ 0.19008625 -1.159731   -0.4801232  -1.1766918 ]\n",
      "  'feature_75'        : [ 0.16258635 -1.7904154  -0.7075715  -1.9496442 ]\n",
      "  'feature_76'        : [ 0.15964766 -2.534744   -1.1617154  -2.9998298 ]\n",
      "  'feature_77'        : [ 0.34012786  0.30995438 -1.417731    1.386645  ]\n",
      "  'feature_78'        : [b'1.4507456979416251' b'' b'-2.6460587429873974' b'1.0685096066146995']\n",
      "  'feature_79'        : [-0.8784881 -1.0238895 -4.218375  -1.9315959]\n",
      "  'feature_80'        : [ 1.1287947  -0.5441783  -2.0493286  -0.90875334]\n",
      "  'feature_81'        : [ 1.1260524 -1.0314782 -3.3098557 -1.8308951]\n",
      "  'feature_82'        : [ 0.57258946 -0.96820545 -3.7906535  -1.9624062 ]\n",
      "  'feature_83'        : [ 1.0447994   0.23317476 -3.994922    0.9734926 ]\n",
      "  'feature_84'        : [b'-0.3079218372415604' b'' b'-0.4925961079380734' b'0.43051949032310977']\n",
      "  'feature_85'        : [-1.2382125 -1.2382125 -1.2382125 -1.2382125]\n",
      "  'feature_86'        : [-0.38092324 -1.2945932  -0.17998733 -1.2945932 ]\n",
      "  'feature_87'        : [-0.77685183 -2.3211558  -0.7736033  -2.3211558 ]\n",
      "  'feature_88'        : [-0.20512624 -1.6418601  -0.32255933 -1.6418601 ]\n",
      "  'feature_89'        : [-0.3221525  -1.8356017   0.33129534 -0.13283424]\n",
      "  'feature_90'        : [b'2.275326388634249' b'' b'1.2873183865340492' b'-0.2745554581006485']\n",
      "  'feature_91'        : [-1.5156134 -1.5156134 -1.5156134 -1.5156134]\n",
      "  'feature_92'        : [ 1.4895896 -2.1158895  1.5373718 -2.1158895]\n",
      "  'feature_93'        : [ 1.2377044  -1.7462853   0.79035085 -1.7462853 ]\n",
      "  'feature_94'        : [ 1.4083351  -1.0868864   0.65511334 -1.0868864 ]\n",
      "  'feature_95'        : [ 3.0820506  -3.2152836   3.620335   -0.78320134]\n",
      "  'feature_96'        : [b'-0.5506623077240874' b'' b'-0.1889481129442559' b'-0.1565074533477293']\n",
      "  'feature_97'        : [-0.0947741  3.7580395  1.5715386  4.2342615]\n",
      "  'feature_98'        : [-1.0217785   0.7336086   0.31267253  0.7580044 ]\n",
      "  'feature_99'        : [-0.6887907   1.3976415   0.35607398  1.5803117 ]\n",
      "  'feature_100'       : [-0.37203243  2.645233    1.0677704   3.267813  ]\n",
      "  'feature_101'       : [-0.34511945 -1.665284    0.48391223 -0.38335317]\n",
      "  'feature_102'       : [b'2.3286620974202634' b'' b'2.387534380011826' b'-1.0882664003285043']\n",
      "  'feature_103'       : [1.0507512 1.2850143 6.597611  2.76058  ]\n",
      "  'feature_104'       : [ 0.66662985 -0.4352869   2.4256206  -0.01828402]\n",
      "  'feature_105'       : [0.3432633  0.10320129 3.102981   0.877458  ]\n",
      "  'feature_106'       : [0.88483626 1.1568334  6.7980103  2.975087  ]\n",
      "  'feature_107'       : [ 2.9161553 -3.3536422  4.0569005 -1.0660226]\n",
      "  'feature_108'       : [b'-0.7875129904411106' b'' b'-0.6415603413461544' b'0.2717937567575263']\n",
      "  'feature_109'       : [-2.2215838   2.3516405   0.28333583  2.755335  ]\n",
      "  'feature_110'       : [-0.7066576  -0.0799825  -0.02830499 -0.06444675]\n",
      "  'feature_111'       : [-0.925759    0.49199486 -0.14692572  0.6185732 ]\n",
      "  'feature_112'       : [-1.6593559   1.8498223   0.28477886  2.4343262 ]\n",
      "  'feature_113'       : [-0.45356143 -2.3718898   0.5136083  -0.36286896]\n",
      "  'feature_114'       : [b'2.2016106854566875' b'' b'1.7368117434082913' b'-0.6112027885824428']\n",
      "  'feature_115'       : [0.02386339 0.30616412 4.8791804  1.7944148 ]\n",
      "  'feature_116'       : [ 1.4145254 -1.6411357  2.6136527 -0.9131979]\n",
      "  'feature_117'       : [ 1.0672177  -1.0052143   2.832761    0.10885008]\n",
      "  'feature_118'       : [ 1.336392   -0.13612151  4.515018    1.3782412 ]\n",
      "  'feature_119'       : [ 2.4283907  -2.363134    3.17458    -0.68139297]\n",
      "  'feature_120'       : [-1.1627114   3.472723   -1.8372457   0.22364621]\n",
      "  'feature_121'       : [-1.6218113   0.31911635 -3.3069668  -2.209027  ]\n",
      "  'feature_122'       : [-1.1260793   5.258679   -1.9803789  -0.37628385]\n",
      "  'feature_123'       : [-0.89299226  1.4656149  -2.1557152  -2.4579842 ]\n",
      "  'feature_124'       : [-0.96534634  2.8678696  -1.6116799   0.26491   ]\n",
      "  'feature_125'       : [-1.450294  -0.2239796 -3.4745762 -2.2142406]\n",
      "  'feature_126'       : [-1.3547847  4.27679   -2.7899954  0.3220604]\n",
      "  'feature_127'       : [-0.6429193   0.01353541 -2.4580529  -1.6911477 ]\n",
      "  'feature_128'       : [-1.192186    4.862884   -2.2801924  -0.06521211]\n",
      "  'feature_129'       : [-0.56832427  0.72035456 -1.9848574  -1.8863435 ]\n",
      "  'ts_id'             : [9443  380 5510 4348]\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_batches.take(1):\n",
    "    print(\"'resp': {}\".format(label_batch))\n",
    "    print(\"features:\")\n",
    "    print()\n",
    "    for key, value in feature_batch.items():\n",
    "        print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exact-trinidad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Use the data of an eight-day window priorier of the date we are investigating as input for prediction\\nWINDOW_SIZE = 8\\n\\ndef batch_dataset(dataset):\\n    dataset_batched = dataset.batch(WINDOW_SIZE,drop_remainder=True)\\n    return dataset_batched\\n\\nds_data = tf.data.Dataset.from_tensor_slices(tf.constant(dfdiff.values,dtype = tf.float32))    .window(WINDOW_SIZE,shift=1).flat_map(batch_dataset)\\n\\nds_label = tf.data.Dataset.from_tensor_slices(\\n    tf.constant(dfdiff.values[WINDOW_SIZE:],dtype = tf.float32))\\n\\n#We put all data into one batch for better efficiency since the data volume is small.\\nds_train = tf.data.Dataset.zip((ds_data,ds_label)).batch(38).cache()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Use the data of an eight-day window priorier of the date we are investigating as input for prediction\n",
    "WINDOW_SIZE = 8\n",
    "\n",
    "def batch_dataset(dataset):\n",
    "    dataset_batched = dataset.batch(WINDOW_SIZE,drop_remainder=True)\n",
    "    return dataset_batched\n",
    "\n",
    "ds_data = tf.data.Dataset.from_tensor_slices(tf.constant(dfdiff.values,dtype = tf.float32)) \\\n",
    "   .window(WINDOW_SIZE,shift=1).flat_map(batch_dataset)\n",
    "\n",
    "ds_label = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.constant(dfdiff.values[WINDOW_SIZE:],dtype = tf.float32))\n",
    "\n",
    "#We put all data into one batch for better efficiency since the data volume is small.\n",
    "ds_train = tf.data.Dataset.zip((ds_data,ds_label)).batch(38).cache()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-camel",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Block, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x_input,x):\n",
    "        x_out = tf.maximum((1+x)*x_input[:,-1,:],0.0)\n",
    "        return x_out\n",
    "    \n",
    "    def get_config(self):  \n",
    "        config = super(Block, self).get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "x_input = layers.Input(shape = (None,3),dtype = tf.float32)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x_input)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x)\n",
    "x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x)\n",
    "x = layers.LSTM(3,input_shape=(None,3))(x)\n",
    "x = layers.Dense(3)(x)\n",
    "\n",
    "#We design the following block since the daily increment of confirmed, discharged and deseased cases are equal or larger than zero.\n",
    "#x = tf.maximum((1+x)*x_input[:,-1,:],0.0)\n",
    "x = Block()(x_input,x)\n",
    "model = models.Model(inputs = [x_input],outputs = [x])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
